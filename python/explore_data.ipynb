{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.board_module as bf\n",
    "import modules.tree_module as tf\n",
    "import modules.stockfish_module as sf\n",
    "from ModelSaver import ModelSaver\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor, ceil\n",
    "\n",
    "# from train_nn_evaluator import EvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, datapath, sample_names, indexes=None, log_level=1):\n",
    "    \"\"\"\n",
    "    Dataset containint stockfish evaluations of chess positions. Pass in the\n",
    "    path to the samples, their names, and the indexes to load\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    self.modelsaver = ModelSaver(datapath, log_level=log_level)\n",
    "    self.log_level = log_level\n",
    "    self.positions = []\n",
    "    self.mate_value = 50000 # 50 pawns\n",
    "    self.convert_evals_to_pawns = True\n",
    "    self.use_all_moves = True\n",
    "    self.use_eval_normalisation = False\n",
    "    self.norm_method = \"standard\"\n",
    "    self.norm_factor = None\n",
    "    self.board_dtype = torch.float\n",
    "\n",
    "    self.boards = []\n",
    "    self.evals = []\n",
    "\n",
    "    # automatically get all indexes if not specified\n",
    "    if indexes is None:\n",
    "      indexes = list(range(self.modelsaver.get_recent_file(name=sample_names, \n",
    "                                                           return_int=True) + 1))\n",
    "\n",
    "    for ind in indexes:\n",
    "      newdata = self.modelsaver.load(sample_names, id=ind)\n",
    "      self.positions += newdata\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    if self.log_level >= 1:\n",
    "      print(f\"EvalDataset(): {len(indexes)} files loaded {t2 - t1:.2f} seconds\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.positions)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    if idx > len(self.positions):\n",
    "      raise RuntimeError(f\"EvalDataset.__getitem__() error: idx ({idx}) > number of samples (len{self.positions})\")\n",
    "    \n",
    "    return self.positions[idx]\n",
    "  \n",
    "  def FEN_to_torch(self, fen_string, move=None):\n",
    "    \"\"\"\n",
    "    Convert an FEN string into a torch tensor board representation\n",
    "    \"\"\"\n",
    "\n",
    "    if move is None:\n",
    "      boardvec = bf.FEN_to_board_vectors(fen_string)\n",
    "    else:\n",
    "      boardvec = bf.FEN_and_move_to_board_vectors(fen_string, move)\n",
    "\n",
    "    tensortype = self.board_dtype\n",
    "\n",
    "    t_wP = torch.tensor(boardvec.wP, dtype=tensortype).reshape(8, 8)\n",
    "    t_wN = torch.tensor(boardvec.wN, dtype=tensortype).reshape(8, 8)\n",
    "    t_wB = torch.tensor(boardvec.wB, dtype=tensortype).reshape(8, 8)\n",
    "    t_wR = torch.tensor(boardvec.wR, dtype=tensortype).reshape(8, 8)\n",
    "    t_wQ = torch.tensor(boardvec.wQ, dtype=tensortype).reshape(8, 8)\n",
    "    t_wK = torch.tensor(boardvec.wK, dtype=tensortype).reshape(8, 8)\n",
    "    t_bP = torch.tensor(boardvec.bP, dtype=tensortype).reshape(8, 8)\n",
    "    t_bN = torch.tensor(boardvec.bN, dtype=tensortype).reshape(8, 8)\n",
    "    t_bB = torch.tensor(boardvec.bB, dtype=tensortype).reshape(8, 8)\n",
    "    t_bR = torch.tensor(boardvec.bR, dtype=tensortype).reshape(8, 8)\n",
    "    t_bQ = torch.tensor(boardvec.bQ, dtype=tensortype).reshape(8, 8)\n",
    "    t_bK = torch.tensor(boardvec.bK, dtype=tensortype).reshape(8, 8)\n",
    "    t_wKS = torch.tensor(boardvec.wKS, dtype=tensortype).reshape(8, 8)\n",
    "    t_wQS = torch.tensor(boardvec.wQS, dtype=tensortype).reshape(8, 8)\n",
    "    t_bKS = torch.tensor(boardvec.bKS, dtype=tensortype).reshape(8, 8)\n",
    "    t_bQS = torch.tensor(boardvec.bQS, dtype=tensortype).reshape(8, 8)\n",
    "    t_colour = torch.tensor(boardvec.colour, dtype=tensortype).reshape(8, 8)\n",
    "    t_total_moves = torch.tensor(boardvec.total_moves, dtype=tensortype).reshape(8, 8)\n",
    "    t_no_take_ply = torch.tensor(boardvec.no_take_ply, dtype=tensortype).reshape(8, 8)\n",
    "\n",
    "    board_tensor = torch.stack((\n",
    "      t_wP,\n",
    "      t_wN,\n",
    "      t_wB,\n",
    "      t_wR,\n",
    "      t_wQ,\n",
    "      t_wK,\n",
    "      t_bP,\n",
    "      t_bN,\n",
    "      t_bB,\n",
    "      t_bR,\n",
    "      t_bQ,\n",
    "      t_bK,\n",
    "      t_wKS,\n",
    "      t_wQS,\n",
    "      t_bKS,\n",
    "      t_bQS,\n",
    "      t_colour,\n",
    "      t_total_moves,\n",
    "      t_no_take_ply,\n",
    "    ), dim=0)\n",
    "\n",
    "    return board_tensor\n",
    "  \n",
    "  def print_board_tensor(self, board_tensor):\n",
    "    \"\"\"\n",
    "    Print the elements of the board tensor\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Board tensor shape:\", board_tensor.shape)\n",
    "    print(\"White pawns\", board_tensor[0])\n",
    "    print(\"White knights\", board_tensor[1])\n",
    "    print(\"White bishops\", board_tensor[2])\n",
    "    print(\"White rooks\", board_tensor[3])\n",
    "    print(\"White queen\", board_tensor[4])\n",
    "    print(\"White king\", board_tensor[5])\n",
    "    print(\"Black pawns\", board_tensor[6])\n",
    "    print(\"Black knights\", board_tensor[7])\n",
    "    print(\"Black bishops\", board_tensor[8])\n",
    "    print(\"Black rooks\", board_tensor[9])\n",
    "    print(\"Black queen\", board_tensor[10])\n",
    "    print(\"Black king\", board_tensor[11])\n",
    "    print(\"White castles KS\", board_tensor[12])\n",
    "    print(\"White castles QS\", board_tensor[13])\n",
    "    print(\"Black castles KS\", board_tensor[14])\n",
    "    print(\"Black castles QS\", board_tensor[15])\n",
    "    print(\"colour\", board_tensor[16])\n",
    "    print(\"total moves\", board_tensor[17])\n",
    "    print(\"no take ply\", board_tensor[18])\n",
    "\n",
    "    return\n",
    "\n",
    "  def normalise_evaluations(self):\n",
    "    \"\"\"\n",
    "    Normalise the evaluations to zero mean and unit variance, and save the scaling\n",
    "    \"\"\"\n",
    "    if self.norm_method == \"minmax\":\n",
    "      max_value = torch.max(-1 * torch.min(self.evals), torch.max(self.evals))\n",
    "      self.norm_factor = max_value\n",
    "      self.evals /= self.norm_factor\n",
    "      if self.log_level > 0:\n",
    "        print(f\"Normalised evaluations, maximum value was {max_value}, now is {torch.max(-1 * torch.min(self.evals), torch.max(self.evals))}\")\n",
    "    \n",
    "    elif self.norm_method == \"standard\":\n",
    "      max_value = torch.max(-1 * torch.min(self.evals), torch.max(self.evals))\n",
    "      mean = self.evals.mean()\n",
    "      std = self.evals.std()\n",
    "      self.evals = (self.evals - mean) / std\n",
    "      new_max = torch.max(-1 * torch.min(self.evals), torch.max(self.evals))\n",
    "      self.evals /= new_max\n",
    "      self.norm_factor = (new_max, mean, std)\n",
    "      if self.log_level > 0:\n",
    "        print(f\"Normalised evaluations, max_value = {max_value:.3f} (max used = {new_max:.3f}), mean = {mean.item():.3f}, std = {std.item():.3f}, now max value is {torch.max(-1 * torch.min(self.evals), torch.max(self.evals)):.3f}, mean is {self.evals.mean().item():.3f} and std is {self.evals.std().item():.3f}\")\n",
    "\n",
    "  def denomormalise_evaluation(self, value=None, all=False):\n",
    "    \"\"\"\n",
    "    Convert a single value back to regular units (or do it for all saved values)\n",
    "    \"\"\"\n",
    "\n",
    "    if self.norm_method == \"minmax\":\n",
    "      if all:\n",
    "        self.evals *= self.norm_factor\n",
    "      elif value is not None:\n",
    "        return value * self.norm_factor\n",
    "      else:\n",
    "        raise RuntimeError(\"EvalDataset.denormalise_evaluations() error: all=False and value=None, incorrect function inputs\")\n",
    "    \n",
    "    if self.norm_method == \"standard\":\n",
    "      if all:\n",
    "        self.evals = (self.evals * self.norm_factor[0] * self.norm_factor[2]) + self.norm_factor[1]\n",
    "      elif value is not None:\n",
    "        return (value * self.norm_factor[0] * self.norm_factor[2]) + self.norm_factor[1]\n",
    "      else:\n",
    "        raise RuntimeError(\"EvalDataset.denormalise_evaluations() error: all=False and value=None, incorrect function inputs\")\n",
    "\n",
    "  def to_torch(self):\n",
    "    \"\"\"\n",
    "    Convert dataset into torch tensors\n",
    "    \"\"\"\n",
    "\n",
    "    if len(self.positions) == 0:\n",
    "      print(\"EvalDataset.to_torch() warning: len(self.positions) == 0, nothing done\")\n",
    "      return\n",
    "\n",
    "    # get the shape of the board tensors\n",
    "    example = self.FEN_to_torch(self.positions[0].fen_string)\n",
    "    num_pos = len(self.positions)\n",
    "\n",
    "    if self.use_all_moves:\n",
    "      # count how many positions we will have\n",
    "      num_lines = 0\n",
    "      for i in range(num_pos):\n",
    "        num_lines += len(self.positions[i].move_vector)\n",
    "      self.boards = torch.zeros((num_lines, *example.shape), dtype=example.dtype)\n",
    "      self.evals = torch.zeros(num_lines, dtype=torch.float)\n",
    "      add_ind = 0\n",
    "      error_moves = 0\n",
    "      if self.log_level > 0:\n",
    "        print(f\"self.use_all_moves = True, found {num_lines} lines (emerging from {num_pos} positions)\")\n",
    "    else:\n",
    "      self.boards = torch.zeros((num_pos, *example.shape), dtype=example.dtype)\n",
    "      self.evals = torch.zeros(num_pos, dtype=torch.float)\n",
    "    \n",
    "    for i in range(num_pos):\n",
    "\n",
    "      if self.use_all_moves:\n",
    "        # loop through all moves and add those boards\n",
    "        for j in range(len(self.positions[i].move_vector)):\n",
    "          if self.positions[i].move_vector[j].move_letters == \"pv\":\n",
    "            error_moves += 1\n",
    "            num_lines -= 1\n",
    "            continue\n",
    "          self.boards[add_ind] = self.FEN_to_torch(self.positions[i].fen_string,\n",
    "                                                   self.positions[i].move_vector[j].move_letters)\n",
    "          if self.positions[i].move_vector[j].eval == \"mate\":\n",
    "            if not bf.is_white_next_FEN(self.positions[i].fen_string):\n",
    "              self.evals[add_ind] = -self.mate_value\n",
    "            else:\n",
    "              self.evals[add_ind] = self.mate_value\n",
    "          else:\n",
    "            self.evals[add_ind] = self.positions[i].move_vector[j].eval\n",
    "          if self.convert_evals_to_pawns:\n",
    "            self.evals[add_ind] *= 1e-3\n",
    "          add_ind += 1\n",
    "\n",
    "      else:\n",
    "        self.boards[i] = self.FEN_to_torch(self.positions[i].fen_string)\n",
    "        if self.positions[i].eval == \"mate\":\n",
    "          if bf.is_white_next_FEN(self.positions[i].fen_string):\n",
    "            self.evals[i] = -self.mate_value\n",
    "          else:\n",
    "            self.evals[i] = self.mate_value\n",
    "        else:\n",
    "          self.evals[i] = self.positions[i].eval\n",
    "        if self.convert_evals_to_pawns:\n",
    "          self.evals[i] *= 1e-3\n",
    "\n",
    "    if self.use_all_moves:\n",
    "      if error_moves > 0:\n",
    "        self.boards = self.boards[:num_lines, :, :]\n",
    "        self.evals = self.evals[:num_lines]\n",
    "        if self.log_level > 0:\n",
    "          print(f\"The number of error moves was: {error_moves}, out of {num_lines + error_moves} lines. New vector length = {self.evals.shape}\")\n",
    "\n",
    "    # # for testing only\n",
    "    # print(\"Shape of self.boards\", self.boards.shape)\n",
    "    # x = num_pos // 2\n",
    "    # bf.print_FEN_board(self.positions[x].fen_string)\n",
    "    # self.print_board_tensor(self.boards[x])\n",
    "\n",
    "    if self.use_eval_normalisation:\n",
    "      self.normalise_evaluations()\n",
    "\n",
    "    return\n",
    "  \n",
    "  def check_duplicates(self, remove=False):\n",
    "    \"\"\"\n",
    "    Check the number (and potentially remove) duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # remove duplicates\n",
    "    seen_values = set()\n",
    "    unique_positions = []\n",
    "\n",
    "    for position in self.positions:\n",
    "      if position.fen_string not in seen_values:\n",
    "        seen_values.add(position.fen_string)\n",
    "        unique_positions.append(position)\n",
    "\n",
    "    num_duplicates = len(self.positions) - len(unique_positions)\n",
    "\n",
    "    # now if we want remove the duplicates\n",
    "    if remove: self.positions = unique_positions\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    if self.log_level >= 1:\n",
    "      print(f\"EvalDataset(): {num_duplicates} duplicates found in {t2 - t1:.2f} seconds{', and removed' if remove else ''}\")\n",
    "\n",
    "    return num_duplicates\n",
    "  \n",
    "  def check_mate_positions(self, remove=False):\n",
    "    \"\"\"\n",
    "    Check the number (and potentially remove) mate positions\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    no_mate_positions = []\n",
    "\n",
    "    # loop backwards over all positions checking for mate\n",
    "    for i in range(len(self.positions) - 1, -1, -1):\n",
    "      if self.positions[i].eval != \"mate\":\n",
    "        no_mate_positions.append(self.positions[i])\n",
    "\n",
    "    num_mates = len(self.positions) - len(no_mate_positions)\n",
    "\n",
    "    # now if we want remove the duplicates\n",
    "    if remove: self.positions = no_mate_positions\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    if self.log_level >= 1:\n",
    "      print(f\"EvalDataset(): {num_mates} mate positions found in {t2 - t1:.2f} seconds{', and removed' if remove else ''}\")\n",
    "\n",
    "    return num_mates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_110.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_001.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_002.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_003.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_004.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_005.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_006.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_007.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_008.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_009.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 3.66 seconds\n"
     ]
    }
   ],
   "source": [
    "# load in the entire dataset\n",
    "num_rand = 4096\n",
    "datapath = \"/home/luke/chess/python/gamedata/samples\"\n",
    "eval_file_template = \"random_n={0}_sample\"\n",
    "inds = list(range(1))\n",
    "log_level = 1\n",
    "dataset = EvalDataset(datapath, eval_file_template.format(num_rand),\n",
    "                      indexes=inds, log_level=log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positions = 37447\n",
      "EvalDataset(): 0 duplicates found in 0.02 seconds\n",
      "EvalDataset(): 0 mate positions found in 0.02 seconds\n",
      "Proportion of duplicates = 0.0 %\n",
      "Proportion of mate positions = 0.0 %\n",
      "REMOVEING MATES AND DUPLICATES\n",
      "EvalDataset(): 0 duplicates found in 0.02 seconds, and removed\n",
      "EvalDataset(): 0 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1087464 lines (emerging from 37447 positions)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m num_mates \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mcheck_mate_positions(remove\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m dataset\u001b[38;5;241m.\u001b[39mboard_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[86], line 212\u001b[0m, in \u001b[0;36mEvalDataset.to_torch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m   num_lines \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    211\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboards[add_ind] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFEN_to_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfen_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_vector\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_letters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions[i]\u001b[38;5;241m.\u001b[39mmove_vector[j]\u001b[38;5;241m.\u001b[39meval \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    215\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bf\u001b[38;5;241m.\u001b[39mis_white_next_FEN(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositions[i]\u001b[38;5;241m.\u001b[39mfen_string):\n",
      "Cell \u001b[0;32mIn[86], line 69\u001b[0m, in \u001b[0;36mEvalDataset.FEN_to_torch\u001b[0;34m(self, fen_string, move)\u001b[0m\n\u001b[1;32m     67\u001b[0m t_bN \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(boardvec\u001b[38;5;241m.\u001b[39mbN, dtype\u001b[38;5;241m=\u001b[39mtensortype)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     68\u001b[0m t_bB \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(boardvec\u001b[38;5;241m.\u001b[39mbB, dtype\u001b[38;5;241m=\u001b[39mtensortype)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m t_bR \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboardvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensortype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     70\u001b[0m t_bQ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(boardvec\u001b[38;5;241m.\u001b[39mbQ, dtype\u001b[38;5;241m=\u001b[39mtensortype)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     71\u001b[0m t_bK \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(boardvec\u001b[38;5;241m.\u001b[39mbK, dtype\u001b[38;5;241m=\u001b[39mtensortype)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Total number of positions = {len(dataset)}\")\n",
    "num_duplicates = dataset.check_duplicates()\n",
    "num_mates = dataset.check_mate_positions()\n",
    "print(f\"Proportion of duplicates = {(num_duplicates / len(dataset))*100:.1f} %\")\n",
    "print(f\"Proportion of mate positions = {(num_mates / len(dataset))*100:.1f} %\")\n",
    "\n",
    "# prepare the dataset\n",
    "print(\"REMOVEING MATES AND DUPLICATES\")\n",
    "num_duplicates = dataset.check_duplicates(remove=True)\n",
    "num_mates = dataset.check_mate_positions(remove=True)\n",
    "dataset.board_dtype = torch.float\n",
    "dataset.to_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading set 1 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_001.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_002.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_003.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_004.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_005.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_006.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_007.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_008.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_009.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_010.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 4.88 seconds\n",
      "EvalDataset(): 3511 duplicates found in 0.03 seconds, and removed\n",
      "EvalDataset(): 74 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1084080 lines (emerging from 37375 positions)\n",
      "The number of error moves was: 2, out of 1084080 lines. New vector length = torch.Size([1084078])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_001.lz4 with pickle ... finished\n",
      "Loading set 2 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_011.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_012.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_013.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_014.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_015.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_016.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_017.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_018.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_019.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_020.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 2.71 seconds\n",
      "EvalDataset(): 3404 duplicates found in 0.03 seconds, and removed\n",
      "EvalDataset(): 70 mate positions found in 0.01 seconds, and removed\n",
      "self.use_all_moves = True, found 1100268 lines (emerging from 37486 positions)\n",
      "The number of error moves was: 2, out of 1100268 lines. New vector length = torch.Size([1100266])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_002.lz4 with pickle ... finished\n",
      "Loading set 3 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_021.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_022.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_023.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_024.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_025.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_026.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_027.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_028.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_029.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_030.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 4.90 seconds\n",
      "EvalDataset(): 3421 duplicates found in 0.03 seconds, and removed\n",
      "EvalDataset(): 78 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1096574 lines (emerging from 37461 positions)\n",
      "The number of error moves was: 5, out of 1096574 lines. New vector length = torch.Size([1096569])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_003.lz4 with pickle ... finished\n",
      "Loading set 4 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_031.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_032.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_033.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_034.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_035.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_036.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_037.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_038.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_039.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_040.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 5.35 seconds\n",
      "EvalDataset(): 3382 duplicates found in 0.04 seconds, and removed\n",
      "EvalDataset(): 90 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1086038 lines (emerging from 37488 positions)\n",
      "The number of error moves was: 1, out of 1086038 lines. New vector length = torch.Size([1086037])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_004.lz4 with pickle ... finished\n",
      "Loading set 5 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_041.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_042.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_043.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_044.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_045.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_046.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_047.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_048.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_049.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_050.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 5.95 seconds\n",
      "EvalDataset(): 3431 duplicates found in 0.04 seconds, and removed\n",
      "EvalDataset(): 81 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1096983 lines (emerging from 37448 positions)\n",
      "The number of error moves was: 4, out of 1096983 lines. New vector length = torch.Size([1096979])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_005.lz4 with pickle ... finished\n",
      "Loading set 6 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_051.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_052.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_053.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_054.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_055.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_056.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_057.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_058.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_059.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_060.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 4.67 seconds\n",
      "EvalDataset(): 3372 duplicates found in 0.03 seconds, and removed\n",
      "EvalDataset(): 78 mate positions found in 0.01 seconds, and removed\n",
      "self.use_all_moves = True, found 1098835 lines (emerging from 37510 positions)\n",
      "The number of error moves was: 2, out of 1098835 lines. New vector length = torch.Size([1098833])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_006.lz4 with pickle ... finished\n",
      "Loading set 7 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_061.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_062.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_063.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_064.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_065.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_066.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_067.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_068.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_069.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_070.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 5.30 seconds\n",
      "EvalDataset(): 3376 duplicates found in 0.04 seconds, and removed\n",
      "EvalDataset(): 92 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1093819 lines (emerging from 37492 positions)\n",
      "The number of error moves was: 3, out of 1093819 lines. New vector length = torch.Size([1093816])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_007.lz4 with pickle ... finished\n",
      "Loading set 8 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_071.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_072.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_073.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_074.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_075.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_076.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_077.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_078.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_079.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_080.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 4.94 seconds\n",
      "EvalDataset(): 3440 duplicates found in 0.03 seconds, and removed\n",
      "EvalDataset(): 79 mate positions found in 0.01 seconds, and removed\n",
      "self.use_all_moves = True, found 1095408 lines (emerging from 37441 positions)\n",
      "The number of error moves was: 3, out of 1095408 lines. New vector length = torch.Size([1095405])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_008.lz4 with pickle ... finished\n",
      "Loading set 9 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_081.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_082.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_083.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_084.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_085.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_086.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_087.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_088.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_089.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_090.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 3.50 seconds\n",
      "EvalDataset(): 3328 duplicates found in 0.04 seconds, and removed\n",
      "EvalDataset(): 84 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1096905 lines (emerging from 37548 positions)\n",
      "The number of error moves was: 2, out of 1096905 lines. New vector length = torch.Size([1096903])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_009.lz4 with pickle ... finished\n",
      "Loading set 10 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_091.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_092.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_093.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_094.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_095.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_096.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_097.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_098.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_099.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_100.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 3.30 seconds\n",
      "EvalDataset(): 3490 duplicates found in 0.04 seconds, and removed\n",
      "EvalDataset(): 90 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1091647 lines (emerging from 37380 positions)\n",
      "The number of error moves was: 3, out of 1091647 lines. New vector length = torch.Size([1091644])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_010.lz4 with pickle ... finished\n",
      "Loading set 11 / 10\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_101.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_102.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_103.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_104.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_105.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_106.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_107.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_108.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_109.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_110.lz4 with pickle ... finished\n",
      "EvalDataset(): 10 files loaded 5.04 seconds\n",
      "EvalDataset(): 3366 duplicates found in 0.04 seconds, and removed\n",
      "EvalDataset(): 92 mate positions found in 0.02 seconds, and removed\n",
      "self.use_all_moves = True, found 1099599 lines (emerging from 37502 positions)\n",
      "The number of error moves was: 2, out of 1099599 lines. New vector length = torch.Size([1099597])\n",
      "Saving file /home/luke/chess/python/datasets/datasetv1_011.lz4 with pickle ... finished\n"
     ]
    }
   ],
   "source": [
    "savenew = False\n",
    "savetorchonly = False\n",
    "\n",
    "if savenew:\n",
    "\n",
    "  dataset_name = \"datasetv1\"\n",
    "  if savetorchonly: dataset_name += \"_torch\"\n",
    "\n",
    "  datasaver = ModelSaver(\"/home/luke/chess/python/datasets/\")\n",
    "\n",
    "  for ind in range(11):\n",
    "\n",
    "    print(\"Loading set\", ind + 1, \"/\", 10)\n",
    "    indexes = list(range(ind * 10 + 1, ((ind + 1) * 10) + 1))\n",
    "    log_level = 1\n",
    "    dataset = EvalDataset(datapath, eval_file_template.format(num_rand),\n",
    "                          indexes=indexes, log_level=log_level)\n",
    "    num_duplicates = dataset.check_duplicates(remove=True)\n",
    "    num_mates = dataset.check_mate_positions(remove=True)\n",
    "    dataset.board_dtype = torch.float\n",
    "    dataset.to_torch()\n",
    "    if savetorchonly:\n",
    "      datasaver.save(dataset_name, [dataset.boards, dataset.evals])\n",
    "    else:\n",
    "      datasaver.save(dataset_name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0530)\n",
      "tensor(-8.1470)\n"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1)\n",
    "axs.hist(dataset.evals.numpy(), bins=50)\n",
    "plt.show()\n",
    "\n",
    "print(torch.max(dataset.evals))\n",
    "print(torch.min(dataset.evals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoardCNN(nn.Module):\n",
    "\n",
    "  name = \"BoardCNN\"\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    super(BoardCNN, self).__init__()\n",
    "\n",
    "    self.board_cnn = nn.Sequential(\n",
    "\n",
    "      # Layer 1\n",
    "      nn.Conv2d(in_channels=19, out_channels=32, kernel_size=3, padding=1),  # Conv layer\n",
    "      nn.ReLU(),                                                             # Activation\n",
    "      nn.MaxPool2d(kernel_size=2),                                           # Pooling (output size: 32 x 4 x 4)\n",
    "\n",
    "      # Layer 2\n",
    "      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2),                                           # Pooling (output size: 64 x 2 x 2)\n",
    "\n",
    "      # Layer 3\n",
    "      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2),                                           # Pooling (output size: 128 x 1 x 1)\n",
    "\n",
    "      # Flatten layer to transition to fully connected\n",
    "      nn.Flatten(),\n",
    "\n",
    "      # two fully connected layers to produce a single output\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 1),\n",
    "  )\n",
    "\n",
    "  def forward(self, board):\n",
    "    board = board.to(self.device)\n",
    "    x = self.board_cnn(board)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "\n",
    "    super(ResidualBlock, self).__init__()\n",
    "    \n",
    "    self.conv1 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.conv2 = nn.Sequential(\n",
    "      nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "      nn.BatchNorm2d(out_channels)\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "    self.relu = nn.ReLU()\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "  def forward(self, x):\n",
    "    residual = x\n",
    "    out = self.conv1(x)\n",
    "    out = self.conv2(out)\n",
    "    if self.downsample:\n",
    "      residual = self.downsample(x)\n",
    "    out += residual\n",
    "    out = self.relu(out)\n",
    "    return out\n",
    "  \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "  def __init__(self, block, layers, num_classes = 10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.inplanes = 64\n",
    "    self.conv1 = nn.Sequential(\n",
    "      nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "    self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "    self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "    self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "    self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "    self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "    self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, blocks, stride=1):\n",
    "    downsample = None\n",
    "    if stride != 1 or self.inplanes != planes:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "          nn.BatchNorm2d(planes),\n",
    "      )\n",
    "    layers = []\n",
    "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "    self.inplanes = planes\n",
    "    for i in range(1, blocks):\n",
    "      layers.append(block(self.inplanes, planes))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.maxpool(x)\n",
    "    x = self.layer0(x)\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "\n",
    "    x = self.avgpool(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.fc(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class ChessNet(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels):\n",
    "\n",
    "    super(ChessNet, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    c_in = in_channels\n",
    "\n",
    "    self.board_cnn = nn.Sequential(\n",
    "      ResidualBlock(c_in, c_in),\n",
    "      ResidualBlock(c_in, c_in),\n",
    "      ResidualBlock(c_in, c_in),\n",
    "      nn.Sequential(nn.Flatten(), nn.Linear(c_in * 8 * 8, c_in), nn.ReLU()),\n",
    "      nn.Linear(c_in, 1),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    for l in self.layers:\n",
    "      x = l(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_x, data_y, epochs=1, lr=5e-5, device=\"cuda\"):\n",
    "  \"\"\"\n",
    "  Perform a training epoch for a given network based on data inputs\n",
    "  data_x, and correct outputs data_y\n",
    "  \"\"\"\n",
    "\n",
    "  # move onto the specified device\n",
    "  net.board_cnn.to(device)\n",
    "  data_x = data_x.to(device)\n",
    "  data_y = data_y.to(device)\n",
    "\n",
    "  # put the model in training mode\n",
    "  net.board_cnn.train()\n",
    "\n",
    "  lossfcn = nn.MSELoss()\n",
    "  optim = torch.optim.Adam(net.board_cnn.parameters(), lr=lr)\n",
    "\n",
    "  batch_size = 64\n",
    "  num_batches = len(data_x) // batch_size\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "    print(f\"Starting epoch {i + 1}. There will be {num_batches} batches\")\n",
    "\n",
    "    rand_idx = torch.randperm(data_x.shape[0])\n",
    "    avg_loss = 0\n",
    "\n",
    "    for n in range(num_batches):\n",
    "\n",
    "      batch_x = data_x[rand_idx[n * batch_size : (n+1) * batch_size]]\n",
    "      batch_y = data_y[rand_idx[n * batch_size : (n+1) * batch_size]]\n",
    "\n",
    "      # use the model for a prediction and calculate loss\n",
    "      net_y = net.board_cnn(batch_x)\n",
    "      loss = lossfcn(net_y.squeeze(1), batch_y)\n",
    "\n",
    "      # backpropagate\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "      optim.zero_grad()\n",
    "\n",
    "      avg_loss += loss.item()\n",
    "\n",
    "      # if n % 500 == 0:\n",
    "      #   print(f\"Loss is {(avg_loss / (n + 1)) * 1000:.3f}, epoch {i + 1}, batch {n + 1} / {num_batches}\")\n",
    "    \n",
    "    print(f\"Loss is {(avg_loss / (num_batches * batch_size)) * 1000:.3f}, at end of epoch {i + 1}\")\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of self.boards torch.Size([1087462, 19, 8, 8])\n",
      "Board evaluation: -0.049132201820611954\n"
     ]
    }
   ],
   "source": [
    "# for testing only\n",
    "print(\"Shape of self.boards\", dataset.boards.shape)\n",
    "x = 0\n",
    "# bf.print_FEN_board(dataset.positions[x].fen_string)\n",
    "# dataset.print_board_tensor(dataset.boards[x])\n",
    "print(f\"Board evaluation: {dataset.evals[x]}\")\n",
    "\n",
    "# for j in range(len(dataset.positions[x].move_vector)):\n",
    "#   print(dataset.positions[x].move_vector[j].eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "rexamine = False\n",
    "\n",
    "if rexamine:\n",
    "\n",
    "  # load all of the dataset files to examine the data distribution\n",
    "  max_values = []\n",
    "  mean_values = []\n",
    "  std_values = []\n",
    "  for i in range(1, 11):\n",
    "    this_data = datasaver.load(\"datasetv1\", id=i)\n",
    "    this_data.normalise_evaluations()\n",
    "    max_values.append(this_data.norm_factor[0])\n",
    "    mean_values.append(this_data.norm_factor[1])\n",
    "    std_values.append(this_data.norm_factor[2])\n",
    "\n",
    "  true_max = np.max(max_values)\n",
    "  avg_mean = np.mean(mean_values)\n",
    "  avg_std = np.mean(std_values)\n",
    "\n",
    "  print(f\"True max = {true_max:.3f}, true mean = {avg_mean:.3f}, average std = {avg_std:.3f}\")\n",
    "\n",
    "  norm_factors = [true_max, avg_mean, avg_std]\n",
    "\n",
    "else:\n",
    "  # True max = 23.927, true mean = -0.240, average std = 0.355\n",
    "  norm_factors = [23.927, -0.240, 0.355]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1.\n",
      "Loading file /home/luke/chess/python/datasets/datasetv1_001.lz4 with pickle ... finished\n",
      "Starting slice 1 / 10. There will be 16938 batches. Loss is -0.873, during epoch 1, slice 1 / 10\n",
      "Loading file /home/luke/chess/python/datasets/datasetv1_002.lz4 with pickle ... finished\n",
      "Starting slice 2 / 10. There will be 17191 batches. Loss is -0.466, during epoch 1, slice 2 / 10\n",
      "Loading file /home/luke/chess/python/datasets/datasetv1_003.lz4 with pickle ... finished\n",
      "Starting slice 3 / 10. There will be 17133 batches. Loss is -0.301, during epoch 1, slice 3 / 10\n",
      "Loading file /home/luke/chess/python/datasets/datasetv1_004.lz4 with pickle ... finished\n",
      "Starting slice 4 / 10. There will be 16969 batches. Loss is -0.250, during epoch 1, slice 4 / 10\n",
      "Loading file /home/luke/chess/python/datasets/datasetv1_005.lz4 with pickle ... finished\n",
      "Starting slice 5 / 10. There will be 17140 batches. Loss is -0.242, during epoch 1, slice 5 / 10\n",
      "Loading file /home/luke/chess/python/datasets/datasetv1_006.lz4 with pickle ... finished\n",
      "Starting slice 6 / 10. There will be 17169 batches. Loss is -0.241, during epoch 1, slice 6 / 10\n",
      "Loading file /home/luke/chess/python/datasets/datasetv1_007.lz4 with pickle ... finished\n",
      "Starting slice 7 / 10. There will be 17090 batches. "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m data_inds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m))\n\u001b[1;32m    107\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-7\u001b[39m\n\u001b[0;32m--> 109\u001b[0m trained_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_procedure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdataname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasetv1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelSaver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/luke/chess/python/datasets/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdata_inds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnorm_factors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m23.927\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.240\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.355\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[122], line 75\u001b[0m, in \u001b[0;36mtrain_procedure\u001b[0;34m(net, dataname, dataloader, data_inds, norm_factors, epochs, lr, device)\u001b[0m\n\u001b[1;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfcn(net_y\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), batch_y)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# backpropagate\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     77\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/pyenv/py38_general/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pyenv/py38_general/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def normalise_data(data, factors):\n",
    "  \"\"\"\n",
    "  Normalise data based on [max, mean, std]\n",
    "  \"\"\"\n",
    "  max, mean, std = factors\n",
    "  return ((data - mean) / std) / max\n",
    "\n",
    "def denormalise_data(data, factors):\n",
    "  \"\"\"\n",
    "  Undo normalisation based on [max, mean, std]\n",
    "  \"\"\"\n",
    "  max, mean, std = factors\n",
    "  return (data * max * std) + mean\n",
    "\n",
    "def train_procedure(net, dataname, dataloader, data_inds, norm_factors,\n",
    "                    epochs=1, lr=1e-7, device=\"cuda\", batch_size=64,\n",
    "                    loss_style=\"MSE\"):\n",
    "  \"\"\"\n",
    "  Perform a training epoch for a given network based on data inputs\n",
    "  data_x, and correct outputs data_y\n",
    "  \"\"\"\n",
    "\n",
    "  # move onto the specified device\n",
    "  net.board_cnn.to(device)\n",
    "\n",
    "  # put the model in training mode\n",
    "  net.board_cnn.train()\n",
    "\n",
    "  if loss_style.lower() == \"mse\":\n",
    "    lossfcn = nn.MSELoss()\n",
    "  elif loss_style.lower() == \"l1\":\n",
    "    lossfcn = nn.L1Loss()\n",
    "  elif loss_style.lower() == \"huber\":\n",
    "    lossfcn == nn.HuberLoss()\n",
    "  else:\n",
    "    raise RuntimeError(f\"train_procedure() error: loss_style = {loss_style} not recognised\")\n",
    "\n",
    "  optim = torch.optim.Adam(net.board_cnn.parameters(), lr=lr)\n",
    "  \n",
    "  # each epoch, cover the entire training dataset\n",
    "  for i in range(epochs):\n",
    "\n",
    "    print(f\"Starting epoch {i + 1}.\")\n",
    "    total_batches = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # load the dataset in a series of slices\n",
    "    for slice_num, j in enumerate(data_inds):\n",
    "\n",
    "      # load this segment of the dataset\n",
    "      dataset = dataloader.load(dataname, id=j)\n",
    "      data_x = dataset.boards\n",
    "      data_y = dataset.evals\n",
    "\n",
    "      # normalise y labels\n",
    "      data_y = normalise_data(data_y, norm_factors)\n",
    "\n",
    "      num_batches = len(data_x) // batch_size\n",
    "      total_batches += num_batches\n",
    "      rand_idx = torch.randperm(data_x.shape[0])\n",
    "      avg_loss = 0\n",
    "\n",
    "      print(f\"Starting slice {slice_num + 1} / {len(data_inds)}. There will be {num_batches} batches. \", end=\"\", flush=True)\n",
    "\n",
    "      # iterate through each batch for this slice of the dataset\n",
    "      for n in range(num_batches):\n",
    "\n",
    "        batch_x = data_x[rand_idx[n * batch_size : (n+1) * batch_size]]\n",
    "        batch_y = data_y[rand_idx[n * batch_size : (n+1) * batch_size]]\n",
    "\n",
    "        # go to cuda\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        # use the model for a prediction and calculate loss\n",
    "        net_y = net.board_cnn(batch_x)\n",
    "        loss = lossfcn(net_y.squeeze(1), batch_y)\n",
    "\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # if n % 500 == 0:\n",
    "        #   print(f\"Loss is {(avg_loss / (n + 1)) * 1000:.3f}, epoch {i + 1}, batch {n + 1} / {num_batches}\")\n",
    "\n",
    "      # this dataset slice is finished\n",
    "      epoch_loss += avg_loss\n",
    "      avg_loss = avg_loss / num_batches\n",
    "      avg_loss = avg_loss ** 0.5 * norm_factors[0] * norm_factors[2] # try to scale to original units\n",
    "      print(f\"Loss is {avg_loss:.3f}, during epoch {i + 1}, slice {slice_num + 1} / {len(data_inds)}\", flush=True)\n",
    "  \n",
    "    # this epoch is finished\n",
    "    epoch_loss = epoch_loss / total_batches\n",
    "    epoch_loss = epoch_loss ** 0.5 * norm_factors[0] * norm_factors[2] # try to scale to original units\n",
    "    print(f\"Epoch {i + 1} has finished after {total_batches} batches. Overall average loss = {epoch_loss:.3f}\", flush=True)\n",
    "\n",
    "  # finally, return the network that we have trained\n",
    "  return net\n",
    "\n",
    "do_train_procedure = True\n",
    "\n",
    "if do_train_procedure:\n",
    "\n",
    "  net = ChessNet(19)\n",
    "\n",
    "  device = \"cuda\"\n",
    "  epochs = 10\n",
    "  data_inds = list(range(1, 11))\n",
    "  lr = 1e-7\n",
    "\n",
    "  trained_net = train_procedure(\n",
    "    net=net,\n",
    "    dataname=\"datasetv1\",\n",
    "    dataloader=ModelSaver(\"/home/luke/chess/python/datasets/\", log_level=1),\n",
    "    data_inds=list(range(1, 11)),\n",
    "    norm_factors=[23.927, -0.240, 0.355], # [max, mean, std]\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    device=device    \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9692)\n",
      "tensor(1.)\n",
      "Starting epoch 1. There will be 16991 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 8.075, at end of epoch 1\n",
      "Starting epoch 2. There will be 16991 batches\n",
      "Loss is 2.674, at end of epoch 2\n",
      "Starting epoch 3. There will be 16991 batches\n",
      "Loss is 2.321, at end of epoch 3\n",
      "Starting epoch 4. There will be 16991 batches\n",
      "Loss is 2.200, at end of epoch 4\n",
      "Starting epoch 5. There will be 16991 batches\n",
      "Loss is 2.124, at end of epoch 5\n",
      "Starting epoch 6. There will be 16991 batches\n",
      "Loss is 2.067, at end of epoch 6\n",
      "Starting epoch 7. There will be 16991 batches\n",
      "Loss is 2.024, at end of epoch 7\n",
      "Starting epoch 8. There will be 16991 batches\n",
      "Loss is 1.991, at end of epoch 8\n",
      "Starting epoch 9. There will be 16991 batches\n",
      "Loss is 1.962, at end of epoch 9\n",
      "Starting epoch 10. There will be 16991 batches\n",
      "Loss is 1.940, at end of epoch 10\n",
      "Saving file /home/luke/chess/python/models/eval_model_016.lz4 with pickle ... finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/luke/chess/python/models/eval_model_016.lz4'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_training = False\n",
    "\n",
    "if test_training:\n",
    "\n",
    "  # net = BoardCNN()\n",
    "  net = ChessNet(19)\n",
    "\n",
    "  device = \"cuda\"\n",
    "  epochs = 10\n",
    "  lr = 1e-7\n",
    "\n",
    "  # # normalise the evaluations\n",
    "  # print(torch.min(dataset.evals))\n",
    "  # print(torch.max(dataset.evals))\n",
    "  # dataset.evals /= torch.max(dataset.evals)\n",
    "  print(torch.min(dataset.evals))\n",
    "  print(torch.max(dataset.evals))\n",
    "\n",
    "  trained_net = train(net, dataset.boards, dataset.evals, device=device, epochs=epochs, lr=lr)\n",
    "  modelsaver = ModelSaver(\"/home/luke/chess/python/models/\")\n",
    "  modelsaver.save(\"eval_model\", trained_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A loss of 0.03125 corresponds to an average evaluation error of -0.147\n"
     ]
    }
   ],
   "source": [
    "loss = 2.0 / 64\n",
    "equiv = dataset.denomormalise_evaluation(value=loss)\n",
    "print(f\"A loss of {loss} corresponds to an average evaluation error of {equiv:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average difference from 100 samples is 0.472\n"
     ]
    }
   ],
   "source": [
    "avg_diff = 0\n",
    "n = 100\n",
    "for i in range(n):\n",
    "  sf_eval = dataset.positions[i].eval\n",
    "  net_eval = trained_net.board_cnn(dataset.boards[i].to(device).unsqueeze(dim=0))\n",
    "  net_eval = net_eval.to(\"cpu\").item()\n",
    "  if dataset.use_eval_normalisation:\n",
    "    net_eval = dataset.denomormalise_evaluation(value=net_eval)\n",
    "  diff = abs(sf_eval*1e-3 - net_eval)\n",
    "  avg_diff += diff\n",
    "  # print(f\"sf_eval = {sf_eval * 1e-3:.3f}, net_eval = {net_eval:.3f}, difference is {diff:.3f}\")\n",
    "\n",
    "print(f\"The average difference from {n} samples is {avg_diff / n:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1. There will be 16991 batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m more_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      2\u001b[0m new_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n\u001b[0;32m----> 3\u001b[0m trained_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmore_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m modelsaver \u001b[38;5;241m=\u001b[39m ModelSaver(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/luke/chess/python/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m modelsaver\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, trained_net)\n",
      "Cell \u001b[0;32mIn[92], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, data_x, data_y, epochs, lr, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# backpropagate\u001b[39;00m\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 39\u001b[0m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     42\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/pyenv/py38_general/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/pyenv/py38_general/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/pyenv/py38_general/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/pyenv/py38_general/lib/python3.8/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "extend_training = False\n",
    "\n",
    "if extend_training:\n",
    "\n",
    "  more_epochs = 10\n",
    "  new_lr = 1e-8\n",
    "  trained_net = train(trained_net, dataset.boards, dataset.evals, device=device, epochs=more_epochs, lr=new_lr)\n",
    "  modelsaver = ModelSaver(\"/home/luke/chess/python/models/\")\n",
    "  modelsaver.save(\"eval_model\", trained_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
