{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.board_module as bf\n",
    "import modules.tree_module as tf\n",
    "import modules.stockfish_module as sf\n",
    "from ModelSaver import ModelSaver\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import floor, ceil\n",
    "\n",
    "# from train_nn_evaluator import EvalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, datapath, sample_names, indexes=None, log_level=1):\n",
    "    \"\"\"\n",
    "    Dataset containint stockfish evaluations of chess positions. Pass in the\n",
    "    path to the samples, their names, and the indexes to load\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    self.modelsaver = ModelSaver(datapath, log_level=log_level)\n",
    "    self.log_level = log_level\n",
    "    self.positions = []\n",
    "    self.seen_values = None\n",
    "\n",
    "    self.mate_value = 50000             # value of a checkmate (units 1000 per pawn)\n",
    "    self.convert_evals_to_pawns = True  # convert evaluations from 1000 per pawn, to 1.0 per pawn\n",
    "    self.use_all_moves = True           # add in all child moves from a positions, instead of the parent\n",
    "    self.eval_squares = True            # evaluate every square in the board using my handcrafted evaluator\n",
    "    self.use_eval_normalisation = False # apply normalisation to all evaluations\n",
    "    self.norm_method = \"standard\"       # normalisation method to use, standard is mean/std scale -1/+1 bound\n",
    "    self.norm_factor = None             # normalisation factors saved for future use\n",
    "    self.board_dtype = torch.float      # datatype to use for torch tensors\n",
    "\n",
    "    self.boards = []\n",
    "    self.evals = []\n",
    "\n",
    "    # automatically get all indexes if not specified\n",
    "    if indexes is None:\n",
    "      indexes = list(range(self.modelsaver.get_recent_file(name=sample_names, \n",
    "                                                           return_int=True) + 1))\n",
    "\n",
    "    for ind in indexes:\n",
    "      newdata = self.modelsaver.load(sample_names, id=ind)\n",
    "      self.positions += newdata\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    if self.log_level >= 1:\n",
    "      print(f\"EvalDataset(): {len(indexes)} files loaded {t2 - t1:.2f} seconds\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.positions)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    if idx > len(self.positions):\n",
    "      raise RuntimeError(f\"EvalDataset.__getitem__() error: idx ({idx}) > number of samples (len{self.positions})\")\n",
    "    \n",
    "    return self.positions[idx]\n",
    "  \n",
    "  def FEN_to_torch(self, fen_string, move=None, eval_sqs=False):\n",
    "    \"\"\"\n",
    "    Convert an FEN string into a torch tensor board representation\n",
    "    \"\"\"\n",
    "\n",
    "    if move is None:\n",
    "      boardvec = bf.FEN_to_board_vectors(fen_string)\n",
    "    else:\n",
    "      if eval_sqs:\n",
    "        boardvec = bf.FEN_move_eval_to_board_vectors(fen_string, move)\n",
    "      else:\n",
    "        boardvec = bf.FEN_and_move_to_board_vectors(fen_string, move)\n",
    "\n",
    "    tensortype = self.board_dtype\n",
    "\n",
    "    t_wP = torch.tensor(boardvec.wP, dtype=tensortype).reshape(8, 8)\n",
    "    t_wN = torch.tensor(boardvec.wN, dtype=tensortype).reshape(8, 8)\n",
    "    t_wB = torch.tensor(boardvec.wB, dtype=tensortype).reshape(8, 8)\n",
    "    t_wR = torch.tensor(boardvec.wR, dtype=tensortype).reshape(8, 8)\n",
    "    t_wQ = torch.tensor(boardvec.wQ, dtype=tensortype).reshape(8, 8)\n",
    "    t_wK = torch.tensor(boardvec.wK, dtype=tensortype).reshape(8, 8)\n",
    "    t_bP = torch.tensor(boardvec.bP, dtype=tensortype).reshape(8, 8)\n",
    "    t_bN = torch.tensor(boardvec.bN, dtype=tensortype).reshape(8, 8)\n",
    "    t_bB = torch.tensor(boardvec.bB, dtype=tensortype).reshape(8, 8)\n",
    "    t_bR = torch.tensor(boardvec.bR, dtype=tensortype).reshape(8, 8)\n",
    "    t_bQ = torch.tensor(boardvec.bQ, dtype=tensortype).reshape(8, 8)\n",
    "    t_bK = torch.tensor(boardvec.bK, dtype=tensortype).reshape(8, 8)\n",
    "    t_wKS = torch.tensor(boardvec.wKS, dtype=tensortype).reshape(8, 8)\n",
    "    t_wQS = torch.tensor(boardvec.wQS, dtype=tensortype).reshape(8, 8)\n",
    "    t_bKS = torch.tensor(boardvec.bKS, dtype=tensortype).reshape(8, 8)\n",
    "    t_bQS = torch.tensor(boardvec.bQS, dtype=tensortype).reshape(8, 8)\n",
    "    t_colour = torch.tensor(boardvec.colour, dtype=tensortype).reshape(8, 8)\n",
    "\n",
    "    # ignore these as no data in them currently, just wasted space\n",
    "    # t_total_moves = torch.tensor(boardvec.total_moves, dtype=tensortype).reshape(8, 8)\n",
    "    # t_no_take_ply = torch.tensor(boardvec.no_take_ply, dtype=tensortype).reshape(8, 8)\n",
    "\n",
    "    board_tensor = torch.stack((\n",
    "      t_wP,\n",
    "      t_wN,\n",
    "      t_wB,\n",
    "      t_wR,\n",
    "      t_wQ,\n",
    "      t_wK,\n",
    "      t_bP,\n",
    "      t_bN,\n",
    "      t_bB,\n",
    "      t_bR,\n",
    "      t_bQ,\n",
    "      t_bK,\n",
    "      t_wKS,\n",
    "      t_wQS,\n",
    "      t_bKS,\n",
    "      t_bQS,\n",
    "      t_colour,\n",
    "      # t_total_moves,\n",
    "      # t_no_take_ply,\n",
    "    ), dim=0)\n",
    "\n",
    "    if eval_sqs:\n",
    "      sq_evals = torch.tensor(boardvec.sq_evals, dtype=float)\n",
    "      if self.convert_evals_to_pawns:\n",
    "        sq_evals *= 1e-3\n",
    "      return board_tensor, sq_evals\n",
    "    else:\n",
    "      return board_tensor\n",
    "  \n",
    "  def print_board_tensor(self, board_tensor):\n",
    "    \"\"\"\n",
    "    Print the elements of the board tensor\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Board tensor shape:\", board_tensor.shape)\n",
    "    print(\"White pawns\", board_tensor[0])\n",
    "    print(\"White knights\", board_tensor[1])\n",
    "    print(\"White bishops\", board_tensor[2])\n",
    "    print(\"White rooks\", board_tensor[3])\n",
    "    print(\"White queen\", board_tensor[4])\n",
    "    print(\"White king\", board_tensor[5])\n",
    "    print(\"Black pawns\", board_tensor[6])\n",
    "    print(\"Black knights\", board_tensor[7])\n",
    "    print(\"Black bishops\", board_tensor[8])\n",
    "    print(\"Black rooks\", board_tensor[9])\n",
    "    print(\"Black queen\", board_tensor[10])\n",
    "    print(\"Black king\", board_tensor[11])\n",
    "    print(\"White castles KS\", board_tensor[12])\n",
    "    print(\"White castles QS\", board_tensor[13])\n",
    "    print(\"Black castles KS\", board_tensor[14])\n",
    "    print(\"Black castles QS\", board_tensor[15])\n",
    "    print(\"colour\", board_tensor[16])\n",
    "    print(\"total moves\", board_tensor[17])\n",
    "    print(\"no take ply\", board_tensor[18])\n",
    "\n",
    "    return\n",
    "\n",
    "  def normalise_evaluations(self):\n",
    "    \"\"\"\n",
    "    Normalise the evaluations to zero mean and unit variance, and save the scaling\n",
    "    \"\"\"\n",
    "    if self.norm_method == \"minmax\":\n",
    "      max_value = torch.max(-1 * torch.min(self.evals), torch.max(self.evals))\n",
    "      self.norm_factor = max_value\n",
    "      self.evals /= self.norm_factor\n",
    "      if self.log_level > 0:\n",
    "        print(f\"Normalised evaluations, maximum value was {max_value}, now is {torch.max(-1 * torch.min(self.evals), torch.max(self.evals))}\")\n",
    "    \n",
    "    elif self.norm_method == \"standard\":\n",
    "      max_value = torch.max(-1 * torch.min(self.evals), torch.max(self.evals))\n",
    "      mean = self.evals.mean()\n",
    "      std = self.evals.std()\n",
    "      self.evals = (self.evals - mean) / std\n",
    "      new_max = torch.max(-1 * torch.min(self.evals), torch.max(self.evals))\n",
    "      self.evals /= new_max\n",
    "      self.norm_factor = (new_max, mean, std)\n",
    "      if self.log_level > 0:\n",
    "        print(f\"Normalised evaluations, max_value = {max_value:.3f} (max used = {new_max:.3f}), mean = {mean.item():.3f}, std = {std.item():.3f}, now max value is {torch.max(-1 * torch.min(self.evals), torch.max(self.evals)):.3f}, mean is {self.evals.mean().item():.3f} and std is {self.evals.std().item():.3f}\")\n",
    "\n",
    "  def denomormalise_evaluation(self, value=None, all=False):\n",
    "    \"\"\"\n",
    "    Convert a single value back to regular units (or do it for all saved values)\n",
    "    \"\"\"\n",
    "\n",
    "    if self.norm_method == \"minmax\":\n",
    "      if all:\n",
    "        self.evals *= self.norm_factor\n",
    "      elif value is not None:\n",
    "        return value * self.norm_factor\n",
    "      else:\n",
    "        raise RuntimeError(\"EvalDataset.denormalise_evaluations() error: all=False and value=None, incorrect function inputs\")\n",
    "    \n",
    "    if self.norm_method == \"standard\":\n",
    "      if all:\n",
    "        self.evals = (self.evals * self.norm_factor[0] * self.norm_factor[2]) + self.norm_factor[1]\n",
    "      elif value is not None:\n",
    "        return (value * self.norm_factor[0] * self.norm_factor[2]) + self.norm_factor[1]\n",
    "      else:\n",
    "        raise RuntimeError(\"EvalDataset.denormalise_evaluations() error: all=False and value=None, incorrect function inputs\")\n",
    "\n",
    "  def to_torch(self):\n",
    "    \"\"\"\n",
    "    Convert dataset into torch tensors\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    if len(self.positions) == 0:\n",
    "      print(\"EvalDataset.to_torch() warning: len(self.positions) == 0, nothing done\")\n",
    "      return\n",
    "\n",
    "    # get the shape of the board tensors\n",
    "    example = self.FEN_to_torch(self.positions[0].fen_string)\n",
    "    num_pos = len(self.positions)\n",
    "\n",
    "    if self.use_all_moves:\n",
    "      # count how many positions we will have\n",
    "      num_lines = 0\n",
    "      for i in range(num_pos):\n",
    "        num_lines += len(self.positions[i].move_vector)\n",
    "      self.boards = torch.zeros((num_lines, *example.shape), dtype=example.dtype)\n",
    "      self.evals = torch.zeros(num_lines, dtype=torch.float)\n",
    "      if self.eval_squares:\n",
    "        self.square_evals = torch.zeros((num_lines, 64), dtype=torch.float)\n",
    "      add_ind = 0\n",
    "      error_moves = 0\n",
    "      if self.log_level > 0:\n",
    "        print(f\"self.use_all_moves = True, found {num_lines} lines (emerging from {num_pos} positions)\")\n",
    "    else:\n",
    "      self.boards = torch.zeros((num_pos, *example.shape), dtype=example.dtype)\n",
    "      self.evals = torch.zeros(num_pos, dtype=torch.float)\n",
    "    \n",
    "    for i in range(num_pos):\n",
    "\n",
    "      if self.use_all_moves:\n",
    "        # loop through all moves and add those boards\n",
    "        for j in range(len(self.positions[i].move_vector)):\n",
    "          if self.positions[i].move_vector[j].move_letters == \"pv\":\n",
    "            error_moves += 1\n",
    "            num_lines -= 1\n",
    "            continue\n",
    "          if self.eval_squares:\n",
    "            self.boards[add_ind], self.square_evals[add_ind] = self.FEN_to_torch(\n",
    "              self.positions[i].fen_string, self.positions[i].move_vector[j].move_letters, self.eval_squares\n",
    "            )\n",
    "          else:\n",
    "            self.boards[add_ind] = self.FEN_to_torch(self.positions[i].fen_string,\n",
    "                                                    self.positions[i].move_vector[j].move_letters)\n",
    "          if self.positions[i].move_vector[j].eval == \"mate\":\n",
    "            if not bf.is_white_next_FEN(self.positions[i].fen_string):\n",
    "              self.evals[add_ind] = -self.mate_value\n",
    "            else:\n",
    "              self.evals[add_ind] = self.mate_value\n",
    "          else:\n",
    "            self.evals[add_ind] = self.positions[i].move_vector[j].eval\n",
    "          if self.convert_evals_to_pawns:\n",
    "            self.evals[add_ind] *= 1e-3\n",
    "          add_ind += 1\n",
    "\n",
    "      else:\n",
    "        self.boards[i] = self.FEN_to_torch(self.positions[i].fen_string)\n",
    "        if self.positions[i].eval == \"mate\":\n",
    "          if bf.is_white_next_FEN(self.positions[i].fen_string):\n",
    "            self.evals[i] = -self.mate_value\n",
    "          else:\n",
    "            self.evals[i] = self.mate_value\n",
    "        else:\n",
    "          self.evals[i] = self.positions[i].eval\n",
    "        if self.convert_evals_to_pawns:\n",
    "          self.evals[i] *= 1e-3\n",
    "\n",
    "    if self.use_all_moves:\n",
    "      if error_moves > 0:\n",
    "        self.boards = self.boards[:num_lines, :, :]\n",
    "        self.evals = self.evals[:num_lines]\n",
    "        if self.log_level > 0:\n",
    "          print(f\"The number of error moves was: {error_moves}, out of {num_lines + error_moves} lines. New vector length = {self.evals.shape}\")\n",
    "\n",
    "    # # for testing only\n",
    "    # print(\"Shape of self.boards\", self.boards.shape)\n",
    "    # x = num_pos // 2\n",
    "    # bf.print_FEN_board(self.positions[x].fen_string)\n",
    "    # self.print_board_tensor(self.boards[x])\n",
    "\n",
    "    if self.use_eval_normalisation:\n",
    "      if self.log_level > 0:\n",
    "        print(\"EvalDataset() is applying normalisation to self.evals\")\n",
    "      self.normalise_evaluations()\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    if self.log_level > 0:\n",
    "      if self.use_all_moves:\n",
    "        total_num = num_lines\n",
    "      else: total_num = num_pos\n",
    "      print(f\"EvalDataset(): {total_num} positions converted in {t2 - t1:.2f} seconds, average {((t2 - t1) / total_num) * 1e3:.3f} ms per position\")\n",
    "\n",
    "    return\n",
    "  \n",
    "  def check_duplicates(self, remove=False, wipe_seen=True):\n",
    "    \"\"\"\n",
    "    Check the number (and potentially remove) duplicates\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # remove duplicates\n",
    "    if wipe_seen:\n",
    "      self.seen_values = set()\n",
    "    elif self.seen_values is None:\n",
    "      self.seen_values = set()\n",
    "      \n",
    "    unique_positions = []\n",
    "\n",
    "    for position in self.positions:\n",
    "      if position.fen_string not in self.seen_values:\n",
    "        self.seen_values.add(position.fen_string)\n",
    "        unique_positions.append(position)\n",
    "\n",
    "    num_duplicates = len(self.positions) - len(unique_positions)\n",
    "\n",
    "    # now if we want remove the duplicates\n",
    "    if remove: self.positions = unique_positions\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    if self.log_level >= 1:\n",
    "      print(f\"EvalDataset(): {num_duplicates} duplicates found in {t2 - t1:.2f} seconds{', and removed' if remove else ''}\")\n",
    "\n",
    "    return num_duplicates\n",
    "  \n",
    "  def check_mate_positions(self, remove=False):\n",
    "    \"\"\"\n",
    "    Check the number (and potentially remove) mate positions\n",
    "    \"\"\"\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    no_mate_positions = []\n",
    "\n",
    "    # loop backwards over all positions checking for mate\n",
    "    for i in range(len(self.positions) - 1, -1, -1):\n",
    "      if self.positions[i].eval != \"mate\":\n",
    "        no_mate_positions.append(self.positions[i])\n",
    "\n",
    "    num_mates = len(self.positions) - len(no_mate_positions)\n",
    "\n",
    "    # now if we want remove the duplicates\n",
    "    if remove: self.positions = no_mate_positions\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    if self.log_level >= 1:\n",
    "      print(f\"EvalDataset(): {num_mates} mate positions found in {t2 - t1:.2f} seconds{', and removed' if remove else ''}\")\n",
    "\n",
    "    return num_mates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file /home/luke/chess/python/gamedata/samples/random_n=4096_sample_140.lz4 with pickle ... finished\n",
      "EvalDataset(): 1 files loaded 0.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# load in the entire dataset\n",
    "num_rand = 4096\n",
    "datapath = \"/home/luke/chess/python/gamedata/samples\"\n",
    "eval_file_template = \"random_n={0}_sample\"\n",
    "inds = list(range(1))\n",
    "log_level = 1\n",
    "dataset = EvalDataset(datapath, eval_file_template.format(num_rand),\n",
    "                      indexes=inds, log_level=log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positions = 4096\n",
      "EvalDataset(): 186 duplicates found in 0.00 seconds\n",
      "EvalDataset(): 6 mate positions found in 0.00 seconds\n",
      "Proportion of duplicates = 4.5 %\n",
      "Proportion of mate positions = 0.1 %\n",
      "REMOVING MATES AND DUPLICATES\n",
      "EvalDataset(): 186 duplicates found in 0.00 seconds, and removed\n",
      "EvalDataset(): 6 mate positions found in 0.00 seconds, and removed\n",
      "self.use_all_moves = True, found 113778 lines (emerging from 3904 positions)\n",
      "EvalDataset(): 113778 positions converted in 28.18 seconds, average 0.248 ms per position\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of positions = {len(dataset)}\")\n",
    "num_duplicates = dataset.check_duplicates()\n",
    "num_mates = dataset.check_mate_positions()\n",
    "print(f\"Proportion of duplicates = {(num_duplicates / len(dataset))*100:.1f} %\")\n",
    "print(f\"Proportion of mate positions = {(num_mates / len(dataset))*100:.1f} %\")\n",
    "\n",
    "# prepare the dataset\n",
    "print(\"REMOVING MATES AND DUPLICATES\")\n",
    "num_duplicates = dataset.check_duplicates(remove=True)\n",
    "num_mates = dataset.check_mate_positions(remove=True)\n",
    "dataset.board_dtype = torch.float\n",
    "dataset.to_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "savenew = False\n",
    "\n",
    "if savenew:\n",
    "\n",
    "  dataset_name = \"datasetv2\"\n",
    "  file_name = \"data\"\n",
    "  ind_per = 2               # indexes per slice of the dataset\n",
    "  total_index = 140         # largest index number of gamedata/samples file\n",
    "  prevent_duplicates = True # prevent duplicates across the entire set, not just in each slice\n",
    "  savetorchonly = False     # save only the finalised torch tensors\n",
    "  savetorchtoo = True       # save also a torch version of the dataset\n",
    "  log_level = 1             # log level during the dataset generation (0=bare minimum, 1=normal)\n",
    "\n",
    "  num_sets = total_index // ind_per\n",
    "  if prevent_duplicates: seen_values = set()\n",
    "\n",
    "  datasaver = ModelSaver(\"/home/luke/chess/python/datasets/\", log_level=log_level)\n",
    "  datasaver.new_folder(dataset_name)\n",
    "\n",
    "  for ind in range(num_sets):\n",
    "\n",
    "    print(\"Loading set\", ind + 1, \"/\", num_sets)\n",
    "    indexes = list(range(ind * ind_per + 1, ((ind + 1) * ind_per) + 1))\n",
    "    dataset = EvalDataset(datapath, eval_file_template.format(num_rand),\n",
    "                          indexes=indexes, log_level=log_level)\n",
    "    if prevent_duplicates: dataset.seen_values = seen_values\n",
    "    num_duplicates = dataset.check_duplicates(remove=True, wipe_seen=not prevent_duplicates)\n",
    "    num_mates = dataset.check_mate_positions(remove=True)\n",
    "    dataset.board_dtype = torch.float\n",
    "    dataset.to_torch()\n",
    "    if savetorchonly or savetorchtoo:\n",
    "      if dataset.eval_squares:\n",
    "        datasaver.save(file_name + \"_torch\", [dataset.boards, dataset.evals, dataset.square_evals])\n",
    "      else:\n",
    "        datasaver.save(file_name + \"_torch\", [dataset.boards, dataset.evals])\n",
    "    elif not savetorchonly:\n",
    "      datasaver.save(file_name, dataset)\n",
    "    if prevent_duplicates: seen_values = dataset.seen_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "savenew_randomised = False\n",
    "\n",
    "if savenew_randomised:\n",
    "\n",
    "  dataset_name = \"delete\"\n",
    "  file_name = \"data\"\n",
    "  num_per = 200_000         # number of lines per saved file\n",
    "  total_index = 140         # largest index number of gamedata/samples file\n",
    "  prevent_duplicates = True # prevent duplicates across the entire set, not just in each slice\n",
    "  log_level = 1             # log level during the dataset generation (0=bare minimum, 1=normal)\n",
    "\n",
    "  datasaver = ModelSaver(\"/home/luke/chess/python/datasets/\", log_level=log_level)\n",
    "  datasaver.new_folder(dataset_name)\n",
    "\n",
    "  indexes = list(range(1, total_index + 1))\n",
    "  dataset = EvalDataset(datapath, eval_file_template.format(num_rand),\n",
    "                          indexes=indexes, log_level=log_level)\n",
    "  num_duplicates = dataset.check_duplicates(remove=True)\n",
    "  num_mates = dataset.check_mate_positions(remove=True)\n",
    "  dataset.to_torch()\n",
    "\n",
    "  # now randomise a selection of the indexes\n",
    "  num_boards = len(dataset.evals)\n",
    "  indexes = list(range(num_boards))\n",
    "  random.shuffle(indexes)\n",
    "\n",
    "  num_files = num_boards // num_per\n",
    "  ind = 0\n",
    "\n",
    "  print(\"num_files is\", num_files)\n",
    "  print(\"num_boards is\", num_boards)\n",
    "  \n",
    "  for n in range(num_files):\n",
    "\n",
    "    boards = torch.zeros((num_per, *dataset.boards[0].shape))\n",
    "    evals = torch.zeros((num_per))\n",
    "    sq_evals = torch.zeros((num_per, *dataset.square_evals[0].shape))\n",
    "\n",
    "    for i in range(num_per):\n",
    "\n",
    "      boards[i] = dataset.boards[indexes[ind]]\n",
    "      evals[i] = dataset.evals[indexes[ind]]\n",
    "      sq_evals[i] = dataset.square_evals[indexes[ind]]\n",
    "\n",
    "      # increment to the next random index\n",
    "      ind += 1\n",
    "\n",
    "    datasaver.save(file_name + \"_torch\", [boards, evals, sq_evals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(data, factors, clip=None):\n",
    "  \"\"\"\n",
    "  Normalise data based on [max, mean, std]\n",
    "  \"\"\"\n",
    "  max, mean, std = factors\n",
    "  d = ((data - mean) / std) / max\n",
    "  if clip is not None:\n",
    "    d = torch.clip(d, min=-clip, max=clip)\n",
    "  return d\n",
    "\n",
    "examine = False\n",
    "\n",
    "if examine:\n",
    "\n",
    "  factors = [23.927, -0.240, 0.355]\n",
    "  factors = [3, -0.240, 0.355]\n",
    "  norm_evals = normalise_data(dataset.evals, factors, clip=1)\n",
    "\n",
    "  fig, axs = plt.subplots(1, 1)\n",
    "  axs.hist(norm_evals.numpy(), bins=50)\n",
    "  plt.show()\n",
    "\n",
    "  print(torch.max(dataset.evals))\n",
    "  print(torch.min(dataset.evals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoardCNN(nn.Module):\n",
    "\n",
    "  name = \"BoardCNN\"\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    super(BoardCNN, self).__init__()\n",
    "\n",
    "    self.board_cnn = nn.Sequential(\n",
    "\n",
    "      # Layer 1\n",
    "      nn.Conv2d(in_channels=19, out_channels=32, kernel_size=3, padding=1),  # Conv layer\n",
    "      nn.ReLU(),                                                             # Activation\n",
    "      nn.MaxPool2d(kernel_size=2),                                           # Pooling (output size: 32 x 4 x 4)\n",
    "\n",
    "      # Layer 2\n",
    "      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2),                                           # Pooling (output size: 64 x 2 x 2)\n",
    "\n",
    "      # Layer 3\n",
    "      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=2),                                           # Pooling (output size: 128 x 1 x 1)\n",
    "\n",
    "      # Flatten layer to transition to fully connected\n",
    "      nn.Flatten(),\n",
    "\n",
    "      # two fully connected layers to produce a single output\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 1),\n",
    "  )\n",
    "\n",
    "  def forward(self, board):\n",
    "    board = board.to(self.device)\n",
    "    x = self.board_cnn(board)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "\n",
    "    super(ResidualBlock, self).__init__()\n",
    "    \n",
    "    self.conv1 = nn.Sequential(\n",
    "      nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "      nn.BatchNorm2d(out_channels),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.conv2 = nn.Sequential(\n",
    "      nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "      nn.BatchNorm2d(out_channels)\n",
    "    )\n",
    "\n",
    "    self.downsample = downsample\n",
    "    self.relu = nn.ReLU()\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "  def forward(self, x):\n",
    "    residual = x\n",
    "    out = self.conv1(x)\n",
    "    out = self.conv2(out)\n",
    "    if self.downsample:\n",
    "      residual = self.downsample(x)\n",
    "    out += residual\n",
    "    out = self.relu(out)\n",
    "    return out\n",
    "  \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "  def __init__(self, block, layers, num_classes = 10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.inplanes = 64\n",
    "    self.conv1 = nn.Sequential(\n",
    "      nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "    self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
    "    self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
    "    self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
    "    self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
    "    self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "    self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "  def _make_layer(self, block, planes, blocks, stride=1):\n",
    "    downsample = None\n",
    "    if stride != 1 or self.inplanes != planes:\n",
    "      downsample = nn.Sequential(\n",
    "          nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "          nn.BatchNorm2d(planes),\n",
    "      )\n",
    "    layers = []\n",
    "    layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "    self.inplanes = planes\n",
    "    for i in range(1, blocks):\n",
    "      layers.append(block(self.inplanes, planes))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.maxpool(x)\n",
    "    x = self.layer0(x)\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "\n",
    "    x = self.avgpool(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = self.fc(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class ChessNet(nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels, num_blocks=3):\n",
    "\n",
    "    super(ChessNet, self).__init__()\n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    c_in = in_channels\n",
    "\n",
    "    blocks = [ResidualBlock(c_in, c_in) for i in range(num_blocks)]\n",
    "\n",
    "    self.board_cnn = nn.Sequential(\n",
    "      *blocks,\n",
    "      nn.Sequential(nn.Flatten(), nn.Linear(c_in * 8 * 8, c_in), nn.ReLU()),\n",
    "      nn.Linear(c_in, 1),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    for l in self.layers:\n",
    "      x = l(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_x, data_y, epochs=1, lr=5e-5, device=\"cuda\"):\n",
    "  \"\"\"\n",
    "  Perform a training epoch for a given network based on data inputs\n",
    "  data_x, and correct outputs data_y\n",
    "  \"\"\"\n",
    "\n",
    "  # move onto the specified device\n",
    "  net.board_cnn.to(device)\n",
    "  data_x = data_x.to(device)\n",
    "  data_y = data_y.to(device)\n",
    "\n",
    "  # put the model in training mode\n",
    "  net.board_cnn.train()\n",
    "\n",
    "  lossfcn = nn.MSELoss()\n",
    "  optim = torch.optim.Adam(net.board_cnn.parameters(), lr=lr)\n",
    "\n",
    "  batch_size = 64\n",
    "  num_batches = len(data_x) // batch_size\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "    print(f\"Starting epoch {i + 1}. There will be {num_batches} batches\")\n",
    "\n",
    "    rand_idx = torch.randperm(data_x.shape[0])\n",
    "    avg_loss = 0\n",
    "\n",
    "    for n in range(num_batches):\n",
    "\n",
    "      batch_x = data_x[rand_idx[n * batch_size : (n+1) * batch_size]]\n",
    "      batch_y = data_y[rand_idx[n * batch_size : (n+1) * batch_size]]\n",
    "\n",
    "      # use the model for a prediction and calculate loss\n",
    "      net_y = net.board_cnn(batch_x)\n",
    "      loss = lossfcn(net_y.squeeze(1), batch_y)\n",
    "\n",
    "      # backpropagate\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "      optim.zero_grad()\n",
    "\n",
    "      avg_loss += loss.item()\n",
    "\n",
    "      # if n % 500 == 0:\n",
    "      #   print(f\"Loss is {(avg_loss / (n + 1)) * 1000:.3f}, epoch {i + 1}, batch {n + 1} / {num_batches}\")\n",
    "    \n",
    "    print(f\"Loss is {(avg_loss / (num_batches * batch_size)) * 1000:.3f}, at end of epoch {i + 1}\")\n",
    "\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rexamine = False\n",
    "\n",
    "if rexamine:\n",
    "\n",
    "  # load all of the dataset files to examine the data distribution\n",
    "  max_values = []\n",
    "  mean_values = []\n",
    "  std_values = []\n",
    "  for i in range(1, 11):\n",
    "    this_data = datasaver.load(\"datasetv1\", id=i)\n",
    "    this_data.normalise_evaluations()\n",
    "    max_values.append(this_data.norm_factor[0])\n",
    "    mean_values.append(this_data.norm_factor[1])\n",
    "    std_values.append(this_data.norm_factor[2])\n",
    "\n",
    "  true_max = np.max(max_values)\n",
    "  avg_mean = np.mean(mean_values)\n",
    "  avg_std = np.mean(std_values)\n",
    "\n",
    "  print(f\"True max = {true_max:.3f}, true mean = {avg_mean:.3f}, average std = {avg_std:.3f}\")\n",
    "\n",
    "  norm_factors = [true_max, avg_mean, avg_std]\n",
    "\n",
    "else:\n",
    "  # True max = 23.927, true mean = -0.240, average std = 0.355\n",
    "  norm_factors = [23.927, -0.240, 0.355]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(data, factors):\n",
    "  \"\"\"\n",
    "  Normalise data based on [max, mean, std]\n",
    "  \"\"\"\n",
    "  max, mean, std = factors\n",
    "  return ((data - mean) / std) / max\n",
    "\n",
    "def denormalise_data(data, factors):\n",
    "  \"\"\"\n",
    "  Undo normalisation based on [max, mean, std]\n",
    "  \"\"\"\n",
    "  max, mean, std = factors\n",
    "  return (data * max * std) + mean\n",
    "\n",
    "def train_procedure(net, dataname, dataloader, data_inds, norm_factors,\n",
    "                    epochs=1, lr=1e-7, device=\"cuda\", batch_size=64,\n",
    "                    loss_style=\"MSE\"):\n",
    "  \"\"\"\n",
    "  Perform a training epoch for a given network based on data inputs\n",
    "  data_x, and correct outputs data_y\n",
    "  \"\"\"\n",
    "\n",
    "  # move onto the specified device\n",
    "  net.board_cnn.to(device)\n",
    "\n",
    "  # put the model in training mode\n",
    "  net.board_cnn.train()\n",
    "\n",
    "  if loss_style.lower() == \"mse\":\n",
    "    lossfcn = nn.MSELoss()\n",
    "  elif loss_style.lower() == \"l1\":\n",
    "    lossfcn = nn.L1Loss()\n",
    "  elif loss_style.lower() == \"huber\":\n",
    "    lossfcn = nn.HuberLoss()\n",
    "  else:\n",
    "    raise RuntimeError(f\"train_procedure() error: loss_style = {loss_style} not recognised\")\n",
    "\n",
    "  optim = torch.optim.Adam(net.board_cnn.parameters(), lr=lr)\n",
    "  \n",
    "  # each epoch, cover the entire training dataset\n",
    "  for i in range(epochs):\n",
    "\n",
    "    print(f\"Starting epoch {i + 1}.\")\n",
    "    total_batches = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # load the dataset in a series of slices\n",
    "    for slice_num, j in enumerate(data_inds):\n",
    "\n",
    "      # load this segment of the dataset\n",
    "      dataset = dataloader.load(dataname, id=j)\n",
    "      data_x = dataset.boards\n",
    "      data_y = dataset.evals\n",
    "\n",
    "      # import sys\n",
    "      # print(\"The size in bytes of data_x\", sys.getsizeof(data_x.storage()))\n",
    "      # print(\"The size in bytes of data_y\", sys.getsizeof(data_y.storage()))\n",
    "\n",
    "      # normalise y labels\n",
    "      data_y = normalise_data(data_y, norm_factors)\n",
    "\n",
    "      num_batches = len(data_x) // batch_size\n",
    "      total_batches += num_batches\n",
    "      rand_idx = torch.randperm(data_x.shape[0])\n",
    "      avg_loss = 0\n",
    "\n",
    "      print(f\"Starting slice {slice_num + 1} / {len(data_inds)}. There will be {num_batches} batches. \", end=\"\", flush=True)\n",
    "\n",
    "      # iterate through each batch for this slice of the dataset\n",
    "      for n in range(num_batches):\n",
    "\n",
    "        batch_x = data_x[rand_idx[n * batch_size : (n+1) * batch_size]]\n",
    "        batch_y = data_y[rand_idx[n * batch_size : (n+1) * batch_size]]\n",
    "\n",
    "        # go to cuda\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        # use the model for a prediction and calculate loss\n",
    "        net_y = net.board_cnn(batch_x)\n",
    "        loss = lossfcn(net_y.squeeze(1), batch_y)\n",
    "\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # if n % 500 == 0:\n",
    "        #   print(f\"Loss is {(avg_loss / (n + 1)) * 1000:.3f}, epoch {i + 1}, batch {n + 1} / {num_batches}\")\n",
    "\n",
    "      # this dataset slice is finished\n",
    "      epoch_loss += avg_loss\n",
    "      avg_loss = avg_loss / num_batches\n",
    "      avg_loss = avg_loss ** 0.5 * norm_factors[0] * norm_factors[2] # try to scale to original units\n",
    "      print(f\"Loss is {avg_loss:.3f}, during epoch {i + 1}, slice {slice_num + 1} / {len(data_inds)}\", flush=True)\n",
    "  \n",
    "    # this epoch is finished\n",
    "    epoch_loss = epoch_loss / total_batches\n",
    "    epoch_loss = epoch_loss ** 0.5 * norm_factors[0] * norm_factors[2] # try to scale to original units\n",
    "    print(f\"Epoch {i + 1} has finished after {total_batches} batches. Overall average loss = {epoch_loss:.3f}\", flush=True)\n",
    "\n",
    "  # finally, return the network that we have trained\n",
    "  return net\n",
    "\n",
    "do_train_procedure = False\n",
    "\n",
    "if do_train_procedure:\n",
    "\n",
    "  net = ChessNet(19)\n",
    "\n",
    "  device = \"cuda\"\n",
    "  epochs = 10\n",
    "  data_inds = list(range(1, 11))\n",
    "  lr = 1e-7\n",
    "\n",
    "  trained_net = train_procedure(\n",
    "    net=net,\n",
    "    dataname=\"datasetv1\",\n",
    "    dataloader=ModelSaver(\"/home/luke/chess/python/datasets/\", log_level=1),\n",
    "    data_inds=list(range(1, 11)),\n",
    "    norm_factors=[23.927, -0.240, 0.355], # [max, mean, std]\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    device=device    \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_training = False\n",
    "\n",
    "if test_training:\n",
    "\n",
    "  # net = BoardCNN()\n",
    "  net = ChessNet(19)\n",
    "\n",
    "  device = \"cuda\"\n",
    "  epochs = 10\n",
    "  lr = 1e-7\n",
    "\n",
    "  # # normalise the evaluations\n",
    "  # print(torch.min(dataset.evals))\n",
    "  # print(torch.max(dataset.evals))\n",
    "  # dataset.evals /= torch.max(dataset.evals)\n",
    "  print(torch.min(dataset.evals))\n",
    "  print(torch.max(dataset.evals))\n",
    "\n",
    "  trained_net = train(net, dataset.boards, dataset.evals, device=device, epochs=epochs, lr=lr)\n",
    "  modelsaver = ModelSaver(\"/home/luke/chess/python/models/\")\n",
    "  modelsaver.save(\"eval_model\", trained_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file /home/luke/chess/python/models/01-11-24/run_10-31_A1/network_005.lz4 with pickle ... finished\n",
      "Loading file /home/luke/chess/python/datasets/datasetv3/data_torch_065.lz4 with pickle ... finished\n",
      "Case 112231 / 49. sf_eval = 5.290, True evaluation = 7.052, net eval = 7.397, difference = -0.345\n",
      "Case 22777 / 49. sf_eval = -3.040, True evaluation = 8.724, net eval = 9.387, difference = -0.663\n",
      "Case 49070 / 49. sf_eval = -4.490, True evaluation = -0.284, net eval = 0.049, difference = -0.333\n",
      "Case 137693 / 49. sf_eval = -12.380, True evaluation = 6.194, net eval = 5.206, difference = 0.988\n",
      "Case 95334 / 49. sf_eval = 0.250, True evaluation = -5.165, net eval = -5.307, difference = 0.142\n",
      "Case 63780 / 49. sf_eval = -10.510, True evaluation = 3.130, net eval = 3.060, difference = 0.070\n",
      "Case 121444 / 49. sf_eval = -3.060, True evaluation = -0.874, net eval = -1.984, difference = 1.110\n",
      "Case 42976 / 49. sf_eval = 0.000, True evaluation = 1.973, net eval = 0.299, difference = 1.674\n",
      "Case 10247 / 49. sf_eval = -3.810, True evaluation = -1.104, net eval = -2.230, difference = 1.126\n",
      "Case 146337 / 49. sf_eval = 5.160, True evaluation = 2.868, net eval = 3.449, difference = -0.581\n",
      "Case 72240 / 49. sf_eval = -6.230, True evaluation = 0.928, net eval = 1.109, difference = -0.181\n",
      "Case 178305 / 49. sf_eval = -0.130, True evaluation = -0.222, net eval = -0.523, difference = 0.301\n",
      "Case 31867 / 49. sf_eval = -0.180, True evaluation = -0.974, net eval = -2.131, difference = 1.157\n",
      "Case 103678 / 49. sf_eval = -3.920, True evaluation = 0.479, net eval = 1.099, difference = -0.620\n",
      "Case 181279 / 49. sf_eval = -4.080, True evaluation = -0.349, net eval = -0.554, difference = 0.205\n",
      "Case 161112 / 49. sf_eval = 1.820, True evaluation = -2.936, net eval = -2.454, difference = -0.482\n",
      "Case 29932 / 49. sf_eval = -0.110, True evaluation = 0.226, net eval = 1.023, difference = -0.797\n",
      "Case 101914 / 49. sf_eval = -7.360, True evaluation = -9.920, net eval = -1.803, difference = -8.117\n",
      "Case 21425 / 49. sf_eval = 0.000, True evaluation = 6.273, net eval = 3.906, difference = 2.367\n",
      "Case 39861 / 49. sf_eval = -4.490, True evaluation = -0.913, net eval = -0.995, difference = 0.082\n",
      "Case 36881 / 49. sf_eval = -0.370, True evaluation = 0.189, net eval = -0.083, difference = 0.272\n",
      "Case 144854 / 49. sf_eval = -1.720, True evaluation = -0.155, net eval = -0.692, difference = 0.537\n",
      "Case 24599 / 49. sf_eval = -0.610, True evaluation = 0.246, net eval = -0.430, difference = 0.676\n",
      "Case 38460 / 49. sf_eval = 3.120, True evaluation = 3.231, net eval = 3.355, difference = -0.124\n",
      "Case 50370 / 49. sf_eval = -0.100, True evaluation = 9.245, net eval = 8.525, difference = 0.720\n",
      "Case 56809 / 49. sf_eval = 1.670, True evaluation = 3.808, net eval = 0.877, difference = 2.931\n",
      "Case 50654 / 49. sf_eval = -6.160, True evaluation = 2.387, net eval = 0.244, difference = 2.143\n",
      "Case 171700 / 49. sf_eval = -2.970, True evaluation = -1.765, net eval = -0.703, difference = -1.062\n",
      "Case 148846 / 49. sf_eval = -1.300, True evaluation = -0.114, net eval = -0.328, difference = 0.214\n",
      "Case 187112 / 49. sf_eval = 0.510, True evaluation = -0.266, net eval = -0.140, difference = -0.126\n",
      "Case 135991 / 49. sf_eval = -4.490, True evaluation = 2.300, net eval = 1.180, difference = 1.120\n",
      "Case 60821 / 49. sf_eval = -0.460, True evaluation = 0.507, net eval = 0.144, difference = 0.363\n",
      "Case 161122 / 49. sf_eval = -2.210, True evaluation = -0.054, net eval = 0.109, difference = -0.163\n",
      "Case 161088 / 49. sf_eval = -5.760, True evaluation = -2.027, net eval = -0.683, difference = -1.344\n",
      "Case 73451 / 49. sf_eval = -7.220, True evaluation = 1.534, net eval = -1.458, difference = 2.992\n",
      "Case 81741 / 49. sf_eval = -2.410, True evaluation = 6.094, net eval = 1.834, difference = 4.260\n",
      "Case 98345 / 49. sf_eval = -0.160, True evaluation = 5.253, net eval = -0.673, difference = 5.926\n",
      "Case 101711 / 49. sf_eval = -0.060, True evaluation = 1.227, net eval = 0.255, difference = 0.972\n",
      "Case 92416 / 49. sf_eval = -0.010, True evaluation = 0.922, net eval = 0.772, difference = 0.150\n",
      "Case 184757 / 49. sf_eval = -2.400, True evaluation = -1.313, net eval = -0.437, difference = -0.876\n",
      "Case 32610 / 49. sf_eval = -4.530, True evaluation = -0.458, net eval = 0.781, difference = -1.239\n",
      "Case 66990 / 49. sf_eval = -1.500, True evaluation = -2.332, net eval = -0.671, difference = -1.661\n",
      "Case 198277 / 49. sf_eval = -1.070, True evaluation = -0.791, net eval = -1.283, difference = 0.492\n",
      "Case 177608 / 49. sf_eval = -3.210, True evaluation = -4.435, net eval = -3.084, difference = -1.351\n",
      "Case 183685 / 49. sf_eval = 0.540, True evaluation = 1.085, net eval = 1.200, difference = -0.115\n",
      "Case 25318 / 49. sf_eval = 0.020, True evaluation = 3.612, net eval = 3.575, difference = 0.037\n",
      "Case 168560 / 49. sf_eval = -1.910, True evaluation = 0.118, net eval = -0.351, difference = 0.469\n",
      "Case 14237 / 49. sf_eval = -2.140, True evaluation = -3.831, net eval = -2.309, difference = -1.522\n",
      "Case 151093 / 49. sf_eval = 0.600, True evaluation = 0.002, net eval = 1.598, difference = -1.596\n",
      "The average difference from 49 samples is 1.159, stockfish average difference is 3.248\n"
     ]
    }
   ],
   "source": [
    "def torch_to_board_vec(tensor):\n",
    "  \"\"\"\n",
    "  Convert a torch board vector into a cpp board vector\n",
    "  \"\"\"\n",
    "\n",
    "  boardvec = bf.BoardVectors()\n",
    "\n",
    "  boardvec.wP = list(tensor[0].reshape(64))\n",
    "  boardvec.wN = list(tensor[1].reshape(64))\n",
    "  boardvec.wB = list(tensor[2].reshape(64))\n",
    "  boardvec.wR = list(tensor[3].reshape(64))\n",
    "  boardvec.wQ = list(tensor[4].reshape(64))\n",
    "  boardvec.wK = list(tensor[5].reshape(64))\n",
    "  boardvec.bP = list(tensor[6].reshape(64))\n",
    "  boardvec.bN = list(tensor[7].reshape(64))\n",
    "  boardvec.bB = list(tensor[8].reshape(64))\n",
    "  boardvec.bR = list(tensor[9].reshape(64))\n",
    "  boardvec.bQ = list(tensor[10].reshape(64))\n",
    "  boardvec.bK = list(tensor[11].reshape(64))\n",
    "  boardvec.wKS = list(tensor[12].reshape(64))\n",
    "  boardvec.wQS = list(tensor[13].reshape(64))\n",
    "  boardvec.bKS = list(tensor[14].reshape(64))\n",
    "  boardvec.bQS = list(tensor[15].reshape(64))\n",
    "  boardvec.colour = list(tensor[16].reshape(64))\n",
    "\n",
    "  return boardvec\n",
    "\n",
    "loadexisting = True\n",
    "\n",
    "if loadexisting:\n",
    "\n",
    "  if False:\n",
    "    modelloader = ModelSaver(\"/home/luke/chess/python/models/\")\n",
    "    trained_net = modelloader.load(\"chessnet_model\")\n",
    "  else:\n",
    "    group = \"01-11-24\"\n",
    "    run = \"run_10-31_A1\"\n",
    "    modelloader = ModelSaver(f\"/home/luke/chess/python/models/{group}/{run}\")\n",
    "    trained_net = modelloader.load(\"network\", id=None)\n",
    "  \n",
    "  dataset_name = \"datasetv3\"\n",
    "  dataset_ind = 65\n",
    "  dataloader = ModelSaver(f\"/home/luke/chess/python/datasets/{dataset_name}\")\n",
    "  boards, evals, sq_evals = dataloader.load(\"data_torch\", id=dataset_ind)\n",
    "\n",
    "  device = \"cpu\"\n",
    "  trained_net.board_cnn.to(device)\n",
    "  boards = boards.to(device)\n",
    "  evals = evals.to(device)\n",
    "  sq_evals = sq_evals.to(device)\n",
    "\n",
    "  trained_net.board_cnn.eval()\n",
    "\n",
    "rand = True\n",
    "inds = list(range(len(evals)))\n",
    "if rand:\n",
    "  random.shuffle(inds)\n",
    "\n",
    "avg_diff_sf = 0\n",
    "avg_diff_my = 0\n",
    "n = 49\n",
    "inds = inds[:n]\n",
    "for i in inds:\n",
    "\n",
    "  # # evaluation comparison\n",
    "  # sf_eval = dataset.positions[i].eval\n",
    "  # net_eval = trained_net.board_cnn(dataset.boards[i].to(device).unsqueeze(dim=0))\n",
    "  # net_eval = net_eval.to(\"cpu\").item()\n",
    "  # if dataset.use_eval_normalisation:\n",
    "  #   net_eval = dataset.denomormalise_evaluation(value=net_eval)\n",
    "  # diff = abs(sf_eval*1e-3 - net_eval)\n",
    "  # avg_diff += diff\n",
    "  # if n <= 20:\n",
    "  #   print(f\"sf_eval = {sf_eval * 1e-3:.3f}, net_eval = {net_eval:.3f}, difference is {diff:.3f}\")\n",
    "\n",
    "  # board piece rating comparison\n",
    "  net_eval = (trained_net.board_cnn(boards[i].unsqueeze(dim=0))).to(\"cpu\")\n",
    "  net_eval = denormalise_data(net_eval, factors=[7, 0, 2.159])\n",
    "\n",
    "  this_board_vec = boards[i].to(\"cpu\")\n",
    "  this_sf_eval = evals[i].to(\"cpu\") * 10\n",
    "  this_sq_evals = sq_evals[i].to(\"cpu\")\n",
    "\n",
    "  # board_vec = bf.FEN_to_board_vectors_with_eval(dataset.positions[i].fen_string)\n",
    "  # board_vec = torch.tensor(board_vec.sq_evals, dtype=torch.float) * 1e-3\n",
    "  \n",
    "  net_overall = torch.sum(net_eval)\n",
    "  true_overall = torch.sum(this_sq_evals)\n",
    "\n",
    "  net_print = torch.zeros((8, 8))\n",
    "  true_print = torch.zeros((8, 8))\n",
    "  \n",
    "  torch.round(net_eval.reshape(8,8).detach(), decimals=2, out=net_print)\n",
    "  torch.round(this_sq_evals.reshape(8,8).detach(), decimals=2, out=true_print)\n",
    "\n",
    "  sf_diff = this_sf_eval - net_overall\n",
    "  my_diff = true_overall - net_overall\n",
    "\n",
    "  avg_diff_sf += abs(sf_diff)\n",
    "  avg_diff_my += abs(my_diff)\n",
    "  \n",
    "  if n <= 50:\n",
    "    print(f\"Case {i + 1} / {n}.\", end=\" \")\n",
    "    if n < 6:\n",
    "      print(\"Board:\")\n",
    "      bf.print_board_vectors(torch_to_board_vec(this_board_vec))\n",
    "      # bf.print_FEN_board(dataset.positions[i].fen_string)\n",
    "      print(\"Net eval was\\n\", net_print)\n",
    "      print(\"Ground truth was\\n\", true_print)\n",
    "    print(f\"sf_eval = {this_sf_eval:.3f}, True evaluation = {true_overall:.3f}, net eval = {net_overall:.3f}, difference = {true_overall - net_overall:.3f}\")\n",
    "\n",
    "\n",
    "print(f\"The average difference from {n} samples is {avg_diff_my / n:.3f}, stockfish average difference is {avg_diff_sf / n:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move: e1g1, eval = 80\n",
      "Move: c1e3, eval = 71\n",
      "Move: h2h4, eval = 70\n",
      "Move: a4a5, eval = 61\n",
      "Move: c1g5, eval = 59\n",
      "Move: e4e5, eval = 59\n",
      "Move: c1f4, eval = 59\n",
      "Move: a1b1, eval = 55\n",
      "Move: d1c2, eval = 36\n",
      "Move: c4b3, eval = 31\n",
      "Move: h2h3, eval = 29\n",
      "Move: c1d2, eval = 26\n",
      "Move: e2g3, eval = 18\n",
      "Move: a1a2, eval = 18\n",
      "Move: a1a3, eval = 14\n",
      "Move: c4a2, eval = 10\n",
      "Move: b2b3, eval = 10\n",
      "Move: c4b5, eval = 9\n",
      "Move: e1f2, eval = 5\n",
      "Move: d1d3, eval = 5\n",
      "Move: b2b4, eval = 3\n",
      "Move: d1b3, eval = 2\n",
      "Move: e2f4, eval = -1\n",
      "Move: c3b5, eval = -1\n",
      "Move: d1d2, eval = -1\n",
      "Move: c4d3, eval = -10\n",
      "Move: g2g4, eval = -19\n",
      "Move: g2g3, eval = -24\n",
      "Move: h1f1, eval = -32\n",
      "Move: c3a2, eval = -38\n",
      "Move: e1f1, eval = -39\n",
      "Move: c3b1, eval = -55\n",
      "Move: e2g1, eval = -56\n",
      "Move: d4d5, eval = -59\n",
      "Move: h1g1, eval = -65\n",
      "Move: e1d2, eval = -169\n",
      "Move: c4d5, eval = -234\n",
      "Move: f3f4, eval = -252\n",
      "Move: c3d5, eval = -261\n",
      "Move: c4f7, eval = -335\n",
      "Move: c4e6, eval = -366\n",
      "Move: c1h6, eval = -391\n",
      "Move: c4a6, eval = -507\n"
     ]
    }
   ],
   "source": [
    "torch.max(evals)\n",
    "x = 13\n",
    "for i in range(len(dataset.positions[x].move_vector)):\n",
    "  print(f\"Move: {dataset.positions[x].move_vector[i].move_letters}, eval = {dataset.positions[x].move_vector[i].eval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network: Total time 0.5s, so 0.497 ms per evaluation\n",
      "Handcrafted: Total time 0.5s, so 0.466 ms per evaluation\n"
     ]
    }
   ],
   "source": [
    "time_net = True\n",
    "\n",
    "if time_net:\n",
    "\n",
    "  timedevice = \"cuda\"\n",
    "  trained_net.board_cnn.eval()\n",
    "  trained_net.board_cnn.to(timedevice)\n",
    "  num = 1000\n",
    "  j = 0\n",
    "\n",
    "  t1 = time.process_time()\n",
    "  for i in range(num):\n",
    "    trained_net.board_cnn(dataset.boards[j].to(timedevice).unsqueeze(dim=0))\n",
    "    j += 1\n",
    "    if j >= len(dataset.boards):\n",
    "      j = 0\n",
    "  t2 = time.process_time()\n",
    "\n",
    "  print(f\"Network: Total time {t2 - t1:.1f}s, so {((t2 - t1) / num) * 1e3:.3f} ms per evaluation\")\n",
    "\n",
    "  t1 = time.process_time()\n",
    "  for i in range(num):\n",
    "    bf.generate_moves_FEN(dataset.positions[j].fen_string)\n",
    "    j += 1\n",
    "    if j >= len(dataset.boards):\n",
    "      j = 0\n",
    "  t2 = time.process_time()\n",
    "\n",
    "  print(f\"Handcrafted: Total time {t2 - t1:.1f}s, so {((t2 - t1) / num) * 1e3:.3f} ms per evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extend_training = False\n",
    "\n",
    "if extend_training:\n",
    "\n",
    "  more_epochs = 10\n",
    "  new_lr = 1e-8\n",
    "  trained_net = train(trained_net, dataset.boards, dataset.evals, device=device, epochs=more_epochs, lr=new_lr)\n",
    "  modelsaver = ModelSaver(\"/home/luke/chess/python/models/\")\n",
    "  modelsaver.save(\"eval_model\", trained_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
